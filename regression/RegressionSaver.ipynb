{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRAPH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to be able to import the graph tensors and restore the trained variables in another session, we should name every single piece of the graph.\n",
    "\n",
    "I would love that naming the inputs `inputs` and the logits `logits` was a convention, so all the models were just black boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 2], name='inputs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = tf.placeholder(tf.float32, [None, 1], name='targets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('first_layer'):\n",
    "    W = tf.Variable(tf.truncated_normal([2, 1], stddev=0.05), name='W')\n",
    "    b = tf.Variable(tf.random_normal([1]), name='b')\n",
    "\n",
    "    output = tf.add(tf.matmul(x, W), b, name='output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logits = tf.identity(output, name='logits')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.00001 # Terribly important hyperparameter. It can make your net go totally crazy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('training'):\n",
    "    cost = tf.reduce_sum(tf.square(logits - y))\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('accuracy'):\n",
    "    accuracy = tf.reduce_mean(y - tf.abs(logits - y)) / tf.reduce_mean(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from helper import get_data\n",
    "\n",
    "inputs, targets = get_data(max_int=10, size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from helper import split_data\n",
    "\n",
    "train_inputs, test_inputs, train_targets, test_targets = split_data(inputs, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from helper import get_batches\n",
    "\n",
    "batch_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a saver object so we can save the graph and variable values.\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1, Batch   1 - Training Loss:  5221.4307\n",
      "Epoch  1, Batch   2 - Training Loss:   334.3157\n",
      "Epoch  1, Batch   3 - Training Loss:    65.3385\n",
      "Epoch  1, Batch   4 - Training Loss:    40.1266\n",
      "Epoch  1, Batch   5 - Training Loss:    36.2257\n",
      "Epoch  1, Batch   6 - Training Loss:    35.3869\n",
      "Epoch  1, Batch   7 - Training Loss:    34.5284\n",
      "Epoch  2, Batch   1 - Training Loss:    32.7147\n",
      "Epoch  2, Batch   2 - Training Loss:    30.2309\n",
      "Epoch  2, Batch   3 - Training Loss:    32.1030\n",
      "Epoch  2, Batch   4 - Training Loss:    30.6360\n",
      "Epoch  2, Batch   5 - Training Loss:    30.1663\n",
      "Epoch  2, Batch   6 - Training Loss:    29.5621\n",
      "Epoch  2, Batch   7 - Training Loss:    29.6601\n",
      "\n",
      "Testing Accuracy: 0.985505223274231\n",
      "\n",
      "The sum of 5 plus 7 is 12.000414848327637\n",
      "\n",
      "The weights are: [[ 0.96216393]\n",
      " [ 0.96657526]]\n",
      "and the bias is: [ 0.42356801]\n",
      "Model saved in file: /tmp/regression_model.ckpt\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # TRAINING\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        batch_x, batch_y = get_batches(train_inputs, train_targets, batch_size)\n",
    "\n",
    "        for batch in range(train_inputs.shape[0]//batch_size):\n",
    "\n",
    "            sess.run(optimizer, feed_dict={\n",
    "                x: batch_x[batch],\n",
    "                y: batch_y[batch]\n",
    "                })\n",
    "\n",
    "            train_loss = sess.run(cost, feed_dict={\n",
    "                x: batch_x[batch],\n",
    "                y: batch_y[batch]\n",
    "                })\n",
    "\n",
    "            print('Epoch {:>2}, Batch {:3} - '\n",
    "                'Training Loss: {:>10.4f}'.format(\n",
    "                    epoch + 1, \n",
    "                    batch + 1,\n",
    "                    train_loss))\n",
    "    \n",
    "\n",
    "    # TESTING\n",
    "    test_accuracy = sess.run(accuracy, feed_dict={\n",
    "        x: test_inputs,\n",
    "        y: test_targets\n",
    "        })\n",
    "    print('\\nTesting Accuracy: {}'.format(test_accuracy))\n",
    "\n",
    "    final_test = sess.run(logits, feed_dict={\n",
    "        x: np.array([[5, 7]])\n",
    "        })\n",
    "    print(\"\\nThe sum of 5 plus 7 is {}\".format(final_test[0][0])) # The result will be near 12.\n",
    "\n",
    "    print(\"\\nThe weights are: {}\".format(sess.run(W)))\n",
    "    print(\"and the bias is: {}\".format(sess.run(b)))\n",
    "    \n",
    "    # SAVING\n",
    "    # Save the graph and variables to disk.\n",
    "    # At first it will be saved in a temporal directory. If we are satisfied with the results, \n",
    "    # we should save all three checkpoint files in a safer directory.\n",
    "    save_path = saver.save(sess, \"/tmp/regression_model.ckpt\")\n",
    "    print(\"\\nModel saved in file: %s\" % save_path)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
