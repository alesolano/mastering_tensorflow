{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRAPH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.truncated_normal([2, 1], stddev=0.05)) # Â¿le ponemos random normal? menos problemas\n",
    "b = tf.Variable(tf.random_normal([1]))\n",
    "\n",
    "output = tf.add(tf.matmul(x, W), b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = tf.placeholder(tf.float32, [None, 1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.00001 # Terribly important hyperparameter. It can make your net go totally crazy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cost = tf.reduce_sum(tf.square(output - y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy = tf.reduce_mean(y - tf.abs(output - y)) / tf.reduce_mean(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from helper import get_data\n",
    "\n",
    "inputs, targets = get_data(max_int=10, size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from helper import split_data\n",
    "\n",
    "train_inputs, test_inputs, train_targets, test_targets = split_data(inputs, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from helper import get_batches\n",
    "\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1, Batch   1 - Training Loss:  7994.3237\n",
      "Epoch  1, Batch   2 - Training Loss:  5432.5186\n",
      "Epoch  1, Batch   3 - Training Loss:  4693.5195\n",
      "Epoch  1, Batch   4 - Training Loss:  3608.9773\n",
      "Epoch  1, Batch   5 - Training Loss:  2830.0552\n",
      "Epoch  1, Batch   6 - Training Loss:  2235.8418\n",
      "Epoch  1, Batch   7 - Training Loss:  1714.8738\n",
      "Epoch  1, Batch   8 - Training Loss:  1239.4219\n",
      "Epoch  1, Batch   9 - Training Loss:  1080.5604\n",
      "Epoch  1, Batch  10 - Training Loss:   893.3036\n",
      "Epoch  1, Batch  11 - Training Loss:   666.0419\n",
      "Epoch  1, Batch  12 - Training Loss:   500.1959\n",
      "Epoch  1, Batch  13 - Training Loss:   394.6442\n",
      "Epoch  1, Batch  14 - Training Loss:   275.8257\n",
      "Epoch  1, Batch  15 - Training Loss:   214.8794\n",
      "Epoch  1, Batch  16 - Training Loss:   214.2600\n",
      "Epoch  1, Batch  17 - Training Loss:   176.8723\n",
      "Epoch  1, Batch  18 - Training Loss:   131.7303\n",
      "Epoch  1, Batch  19 - Training Loss:   105.8717\n",
      "Epoch  1, Batch  20 - Training Loss:   103.7200\n",
      "Epoch  1, Batch  21 - Training Loss:    77.8491\n",
      "Epoch  1, Batch  22 - Training Loss:    70.2373\n",
      "Epoch  1, Batch  23 - Training Loss:    68.9818\n",
      "Epoch  1, Batch  24 - Training Loss:    64.9569\n",
      "Epoch  1, Batch  25 - Training Loss:    48.2936\n",
      "Epoch  1, Batch  26 - Training Loss:    58.2705\n",
      "Epoch  1, Batch  27 - Training Loss:    37.6060\n",
      "Epoch  1, Batch  28 - Training Loss:    38.8018\n",
      "Epoch  1, Batch  29 - Training Loss:    40.8586\n",
      "Epoch  1, Batch  30 - Training Loss:    42.3177\n",
      "Epoch  1, Batch  31 - Training Loss:    37.2026\n",
      "Epoch  1, Batch  32 - Training Loss:    32.6699\n",
      "Epoch  1, Batch  33 - Training Loss:    39.0832\n",
      "Epoch  1, Batch  34 - Training Loss:    34.6902\n",
      "Epoch  1, Batch  35 - Training Loss:    37.1714\n",
      "Epoch  1, Batch  36 - Training Loss:    37.0810\n",
      "Epoch  1, Batch  37 - Training Loss:    36.9273\n",
      "Epoch  1, Batch  38 - Training Loss:    34.2071\n",
      "Epoch  1, Batch  39 - Training Loss:    33.6682\n",
      "Epoch  1, Batch  40 - Training Loss:    34.2896\n",
      "Epoch  1, Batch  41 - Training Loss:    36.5474\n",
      "Epoch  1, Batch  42 - Training Loss:    37.5498\n",
      "Epoch  1, Batch  43 - Training Loss:    43.6510\n",
      "Epoch  1, Batch  44 - Training Loss:    33.0015\n",
      "Epoch  1, Batch  45 - Training Loss:    29.9743\n",
      "Epoch  1, Batch  46 - Training Loss:    34.6061\n",
      "Epoch  1, Batch  47 - Training Loss:    32.3826\n",
      "Epoch  1, Batch  48 - Training Loss:    39.9428\n",
      "Epoch  1, Batch  49 - Training Loss:    40.4627\n",
      "Epoch  1, Batch  50 - Training Loss:    30.0030\n",
      "Epoch  1, Batch  51 - Training Loss:    27.5372\n",
      "Epoch  1, Batch  52 - Training Loss:    31.0117\n",
      "Epoch  1, Batch  53 - Training Loss:    31.7235\n",
      "Epoch  1, Batch  54 - Training Loss:    30.0175\n",
      "Epoch  1, Batch  55 - Training Loss:    32.9839\n",
      "Epoch  1, Batch  56 - Training Loss:    26.7166\n",
      "Epoch  1, Batch  57 - Training Loss:    28.9288\n",
      "Epoch  1, Batch  58 - Training Loss:    35.2580\n",
      "Epoch  1, Batch  59 - Training Loss:    28.7155\n",
      "Epoch  1, Batch  60 - Training Loss:    32.6650\n",
      "Epoch  1, Batch  61 - Training Loss:    28.5243\n",
      "Epoch  1, Batch  62 - Training Loss:    29.7752\n",
      "Epoch  1, Batch  63 - Training Loss:    31.7230\n",
      "Epoch  1, Batch  64 - Training Loss:    32.6822\n",
      "Epoch  1, Batch  65 - Training Loss:    31.8537\n",
      "Epoch  1, Batch  66 - Training Loss:    31.8900\n",
      "Epoch  1, Batch  67 - Training Loss:    31.3069\n",
      "Epoch  1, Batch  68 - Training Loss:    28.4988\n",
      "Epoch  1, Batch  69 - Training Loss:    34.7451\n",
      "Epoch  1, Batch  70 - Training Loss:    26.8644\n",
      "Epoch  1, Batch  71 - Training Loss:    32.3860\n",
      "Epoch  1, Batch  72 - Training Loss:    33.8309\n",
      "Epoch  1, Batch  73 - Training Loss:    31.5105\n",
      "Epoch  1, Batch  74 - Training Loss:    30.3976\n",
      "Epoch  1, Batch  75 - Training Loss:    28.4871\n",
      "Epoch  2, Batch   1 - Training Loss:    26.8423\n",
      "Epoch  2, Batch   2 - Training Loss:    33.2386\n",
      "Epoch  2, Batch   3 - Training Loss:    27.1594\n",
      "Epoch  2, Batch   4 - Training Loss:    31.3916\n",
      "Epoch  2, Batch   5 - Training Loss:    38.2319\n",
      "Epoch  2, Batch   6 - Training Loss:    39.3913\n",
      "Epoch  2, Batch   7 - Training Loss:    32.3425\n",
      "Epoch  2, Batch   8 - Training Loss:    36.6049\n",
      "Epoch  2, Batch   9 - Training Loss:    31.5349\n",
      "Epoch  2, Batch  10 - Training Loss:    27.5153\n",
      "Epoch  2, Batch  11 - Training Loss:    30.7928\n",
      "Epoch  2, Batch  12 - Training Loss:    29.8326\n",
      "Epoch  2, Batch  13 - Training Loss:    30.0601\n",
      "Epoch  2, Batch  14 - Training Loss:    35.7981\n",
      "Epoch  2, Batch  15 - Training Loss:    30.0421\n",
      "Epoch  2, Batch  16 - Training Loss:    25.2801\n",
      "Epoch  2, Batch  17 - Training Loss:    30.7016\n",
      "Epoch  2, Batch  18 - Training Loss:    35.0774\n",
      "Epoch  2, Batch  19 - Training Loss:    26.2200\n",
      "Epoch  2, Batch  20 - Training Loss:    33.0573\n",
      "Epoch  2, Batch  21 - Training Loss:    29.3289\n",
      "Epoch  2, Batch  22 - Training Loss:    27.4731\n",
      "Epoch  2, Batch  23 - Training Loss:    35.6164\n",
      "Epoch  2, Batch  24 - Training Loss:    27.9876\n",
      "Epoch  2, Batch  25 - Training Loss:    26.9592\n",
      "Epoch  2, Batch  26 - Training Loss:    33.8111\n",
      "Epoch  2, Batch  27 - Training Loss:    27.0803\n",
      "Epoch  2, Batch  28 - Training Loss:    31.4679\n",
      "Epoch  2, Batch  29 - Training Loss:    33.7284\n",
      "Epoch  2, Batch  30 - Training Loss:    31.9934\n",
      "Epoch  2, Batch  31 - Training Loss:    28.7499\n",
      "Epoch  2, Batch  32 - Training Loss:    28.6409\n",
      "Epoch  2, Batch  33 - Training Loss:    30.9066\n",
      "Epoch  2, Batch  34 - Training Loss:    28.7234\n",
      "Epoch  2, Batch  35 - Training Loss:    35.7430\n",
      "Epoch  2, Batch  36 - Training Loss:    29.1621\n",
      "Epoch  2, Batch  37 - Training Loss:    30.1928\n",
      "Epoch  2, Batch  38 - Training Loss:    30.0317\n",
      "Epoch  2, Batch  39 - Training Loss:    26.2989\n",
      "Epoch  2, Batch  40 - Training Loss:    25.4657\n",
      "Epoch  2, Batch  41 - Training Loss:    29.5877\n",
      "Epoch  2, Batch  42 - Training Loss:    30.4151\n",
      "Epoch  2, Batch  43 - Training Loss:    32.8485\n",
      "Epoch  2, Batch  44 - Training Loss:    26.6164\n",
      "Epoch  2, Batch  45 - Training Loss:    30.6285\n",
      "Epoch  2, Batch  46 - Training Loss:    32.9693\n",
      "Epoch  2, Batch  47 - Training Loss:    32.9153\n",
      "Epoch  2, Batch  48 - Training Loss:    28.7020\n",
      "Epoch  2, Batch  49 - Training Loss:    34.4756\n",
      "Epoch  2, Batch  50 - Training Loss:    36.8327\n",
      "Epoch  2, Batch  51 - Training Loss:    32.6501\n",
      "Epoch  2, Batch  52 - Training Loss:    30.7299\n",
      "Epoch  2, Batch  53 - Training Loss:    39.9309\n",
      "Epoch  2, Batch  54 - Training Loss:    30.2938\n",
      "Epoch  2, Batch  55 - Training Loss:    34.9425\n",
      "Epoch  2, Batch  56 - Training Loss:    31.8415\n",
      "Epoch  2, Batch  57 - Training Loss:    31.4055\n",
      "Epoch  2, Batch  58 - Training Loss:    29.7822\n",
      "Epoch  2, Batch  59 - Training Loss:    27.2499\n",
      "Epoch  2, Batch  60 - Training Loss:    33.7750\n",
      "Epoch  2, Batch  61 - Training Loss:    27.2165\n",
      "Epoch  2, Batch  62 - Training Loss:    26.0131\n",
      "Epoch  2, Batch  63 - Training Loss:    27.1536\n",
      "Epoch  2, Batch  64 - Training Loss:    32.4718\n",
      "Epoch  2, Batch  65 - Training Loss:    32.6464\n",
      "Epoch  2, Batch  66 - Training Loss:    29.6226\n",
      "Epoch  2, Batch  67 - Training Loss:    23.0568\n",
      "Epoch  2, Batch  68 - Training Loss:    29.1274\n",
      "Epoch  2, Batch  69 - Training Loss:    31.0111\n",
      "Epoch  2, Batch  70 - Training Loss:    28.2514\n",
      "Epoch  2, Batch  71 - Training Loss:    37.2393\n",
      "Epoch  2, Batch  72 - Training Loss:    31.2591\n",
      "Epoch  2, Batch  73 - Training Loss:    29.3212\n",
      "Epoch  2, Batch  74 - Training Loss:    32.2547\n",
      "Epoch  2, Batch  75 - Training Loss:    28.1347\n",
      "\n",
      "Testing Accuracy: 0.9568796753883362\n",
      "\n",
      "The sum of 5 plus 7 is 11.991448402404785\n",
      "\n",
      "The weights are: [[ 0.88418758]\n",
      " [ 0.8903569 ]]\n",
      "and the bias is: [ 1.33801162]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # TRAINING\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        batch_x, batch_y = get_batches(train_inputs, train_targets, batch_size)\n",
    "\n",
    "        for batch in range(train_inputs.shape[0]//batch_size):\n",
    "\n",
    "            sess.run(optimizer, feed_dict={\n",
    "                x: batch_x[batch],\n",
    "                y: batch_y[batch]\n",
    "                })\n",
    "\n",
    "            train_loss = sess.run(cost, feed_dict={\n",
    "                x: batch_x[batch],\n",
    "                y: batch_y[batch]\n",
    "                })\n",
    "\n",
    "            print('Epoch {:>2}, Batch {:3} - '\n",
    "                'Training Loss: {:>10.4f}'.format(\n",
    "                    epoch + 1, \n",
    "                    batch + 1,\n",
    "                    train_loss))\n",
    "    \n",
    "\n",
    "    # TESTING\n",
    "    test_accuracy = sess.run(accuracy, feed_dict={\n",
    "        x: test_inputs,\n",
    "        y: test_targets\n",
    "        })\n",
    "    print('\\nTesting Accuracy: {}'.format(test_accuracy))\n",
    "\n",
    "    final_test = sess.run(output, feed_dict={\n",
    "        x: np.array([[5, 7]])\n",
    "        })\n",
    "    print(\"\\nThe sum of 5 plus 7 is {}\".format(final_test[0][0])) # The result will be near 12.\n",
    "\n",
    "    print(\"\\nThe weights are: {}\".format(sess.run(W)))\n",
    "    print(\"and the bias is: {}\".format(sess.run(b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
